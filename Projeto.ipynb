{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7301be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import traceback\n",
    "import datetime\n",
    "import sys\n",
    "import locale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff6e4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a6c402",
   "metadata": {},
   "source": [
    "# Importar Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37149bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data():\n",
    "    path = \"Datasets/\"#diretório dos datasets\n",
    "    \n",
    "    # --------- as linhas abaixo servem para gerar o nome dos arquivos automaticamente\n",
    "    prefix = \"ABONOP_\"#prefixo comun a todos os arquivos\n",
    "    months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']#meses presentes\n",
    "    years = [\"2017\", \"2018\", \"2019\", \"2020\"]#anos presentes\n",
    "    ext = \".csv\"#extensão dos arquivos\n",
    "    \n",
    "    # ------------------------\n",
    "    \n",
    "    columns = [\"Nome\", \"CPF\", \"Descrição do cargo emprego\", \"Nível de Escolaridade\", \"Denominação do órgão de atuação\",\n",
    "              \"UF da UPAG de vinculação\", \"Denominação unidade organizacional\", \"UF da Residência\",\n",
    "              \"Cidade da residência\", \"Situação servidor\", \"Quantidade de anos no Serviço público\",\n",
    "              \"Quantidade de meses no Serviço público\",\"Ano/Mês inicial DO ABONO DE PERMANENCIA\",\"VAL\"] #colunas presentes\n",
    "    \n",
    "    date_columns = [\"Ano/Mês inicial DO ABONO DE PERMANENCIA\"]#colunas que devem ser transformadas em datas, porém no dataset estão como inteiros\n",
    "    float_columns = {\"VAL\":np.float64}#coluna númerica\n",
    "    \n",
    "    datasets = []#esta variavel vai ser usada como uma lista de DataFrames importados\n",
    "    sep = \";\"#separador utilizado nos arquivos\n",
    "    decimal = \".\"#valores decimais são separados por virgula\n",
    "    try:\n",
    "        name = \"\"\n",
    "        for i in years:# esse loop vai importar os datasets ano a ano\n",
    "            for j in months:# esse loop vai importar os datasets mês a mês\n",
    "                name = path + prefix + j + i + ext# concatena as informações do arquivo para formar seu nome\n",
    "\n",
    "                df = pd.read_csv(name, sep =sep, #low_memory = False,\n",
    "                                names = columns, header = 0,\n",
    "                                #warn_bad_lines = True\n",
    "                                encoding = 'cp1252', index_col=False)#ver documentação do pandas\n",
    "                date = datetime.date(int(i), int(j), 1)#cria uma data do tipo 1/j/i\n",
    "                df[\"DATE\"] = [date for k in range(df.shape[0])]#adiciona uma coluna com a data\n",
    "                datasets.append(df)\n",
    "    except Exception as e:#ocorreu algum exceção\n",
    "        traceback.print_exc()#printa a pilha de execução\n",
    "        print(\"Erro na planilha: {}\".format(name))#printa a plainlha que ocorreu o erro\n",
    "    finally:\n",
    "        if len(datasets) > 0:\n",
    "            datasets = pd.concat(datasets)#concatena os datasets que foram importados com sucesso\n",
    "            datasets.drop_duplicates(subset = columns, inplace = True)#tira as colunas duplicadas, não considera a coluna DATE\n",
    "            \n",
    "            datasets.reset_index(inplace = True)\n",
    "            datasets.drop(columns = [\"index\"], inplace = True)\n",
    "            \n",
    "        return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd8d9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_dataset(name, sep = \",\", columns = []):\n",
    "    df = pd.read_csv(name, sep =sep, #low_memory = False,\n",
    "                    names = columns, header = 0,\n",
    "                    #warn_bad_lines = True\n",
    "                   index_col=False)#ver documentação do pandas\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af4befa",
   "metadata": {},
   "outputs": [],
   "source": [
    "abono_dataset = import_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcfb1fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cidades_dataset = import_dataset(\"Datasets/municipios.csv\", sep = \";\", columns = [\"ConcatUF+Mun\", \"IBGE\", \"IBGE7\", \"UF\", \"Município\", \n",
    "                                               \"Região\", \"População 2010\", \"Porte\", \"Capital\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbdc6bf",
   "metadata": {},
   "source": [
    "# Retirar espaços em branco em excesso e alguns ajustes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038747ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_spaces(df, text_columns, number_columns):\n",
    "    #retira os espaços em branco e coloca todos os caracteres em caixa baixa\n",
    "    df = df.applymap(\n",
    "        lambda x: \" \".join( x.split() ).lower() if isinstance(x, str) else x, \n",
    "        na_action = 'ignore')\n",
    "    #troca todas as ',' por '.'\n",
    "    df[number_columns] = df[number_columns].applymap(\n",
    "        lambda x: \".\".join(x.split(\",\")) if isinstance(x, str) else x, \n",
    "        na_action = 'ignore')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e8dadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_columns = [\"Nome\", \"CPF\", \"Descrição do cargo emprego\", \"Nível de Escolaridade\", \"Denominação do órgão de atuação\",\n",
    "              \"UF da UPAG de vinculação\", \"Denominação unidade organizacional\", \"UF da Residência\",\n",
    "              \"Cidade da residência\", \"Situação servidor\"]\n",
    "number_columns = [\"Quantidade de anos no Serviço público\",\"Quantidade de meses no Serviço público\",\n",
    "                  \"Ano/Mês inicial DO ABONO DE PERMANENCIA\",\"VAL\"]\n",
    "abono_dataset = clean_spaces(abono_dataset, text_columns, number_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fe9543",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    \"Quantidade de anos no Serviço público\": int,\n",
    "    \"Quantidade de meses no Serviço público\": int,\n",
    "    \"VAL\": np.float64\n",
    "}\n",
    "abono_dataset = abono_dataset.astype(dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88819900",
   "metadata": {},
   "source": [
    "# Corrigir coluna Cidade da residência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f4a2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distancia_hamming(string1, string2):\n",
    "    \"\"\"\n",
    "    Calcula a distancia de hamming entre as strings de entrada\n",
    "    \"\"\"\n",
    "    #código de https://stackoverflow.com/questions/54172831/hamming-distance-between-two-strings-in-python\n",
    "    return sum(c1 != c2 for c1, c2 in zip(string1, string2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0138c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrige_cidade(cidades_do_estado , cidade):\n",
    "    \"\"\"\n",
    "    Essa função corrige a cidade dada por uma na lista cidades_do_estado\n",
    "    A função de distancia de hamming é utilizada como critério para escolha\n",
    "    O algoritmo escolhe o elemento com menor distância de hamming da lista cidades_do_estado\n",
    "    com cidade\n",
    "    \"\"\"\n",
    "    if cidade in cidades_do_estado:#verifica se a cidade já existe na lista\n",
    "        return cidade\n",
    "    else:#se não ela vai ser processada\n",
    "        cid_min = \"\"#inicializa a cidade com distancia de hamming minima\n",
    "        distancia_min = 999999#inicializa a minima distancia de hamming encontrada\n",
    "        for cid in cidades_do_estado:#percorre todas as cidades do estado\n",
    "            distancia = distancia_hamming(cid, cidade)#calcula a distancia entre a cidade atual e a cidade informada\n",
    "            if distancia < distancia_min:\n",
    "                #se essa distancia for menor que a minima até agora\n",
    "                #então é feita uma substituição\n",
    "                cid_min = cid\n",
    "                distancia_min = distancia\n",
    "        return cid_min#retorna a cidade com distancia minima encontrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a59871",
   "metadata": {},
   "outputs": [],
   "source": [
    "estados = set(cidades_dataset[\"UF\"].to_list())\n",
    "cidades_por_estado = dict()\n",
    "for estado in estados:#organiza um diciionário de cidades por estado\n",
    "    cds = cidades_dataset.loc[cidades_dataset[\"UF\"] == estado][\"Município\"].to_list()\n",
    "    cidades_por_estado[estado] = list(map(lambda x: x.lower(), cds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafac8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#realiza correção das cidades\n",
    "idx_cidade = 9#indice da coluna cidade\n",
    "idx_uf = 8#indice da coluna uf\n",
    "mudancas = []#esse array vai salvando as correçoes de cada linha\n",
    "cache = dict()#dicionario de cache para poupar processamento\n",
    "for row in abono_dataset.itertuples():\n",
    "    uf = row[idx_uf].upper()#transforma em caixa alta pois as chaves em cidades_por_estado estão em caixa alta, \n",
    "    if uf in cidades_por_estado:# se a uf da linha existe no diciionario de cidades_por_estado\n",
    "        \n",
    "        cd_desconhecida = row[idx_cidade]\n",
    "        cidade = \"\"\n",
    "        if cd_desconhecida in cache:#verifica se a cidade já foi processada\n",
    "            cidade = cache[cd_desconhecida]#recupera o resultado\n",
    "        else:#se não foi\n",
    "            cidades = cidades_por_estado[uf]#obtem as cidades do estado daquela linha\n",
    "            cidade = corrige_cidade(cidades, cd_desconhecida)#corrige a cidade\n",
    "            cache[cd_desconhecida] = cidade#salva o resultado na cache\n",
    "        mudancas.append(cidade)#adiciona na lista de mudancas\n",
    "    else:# se não existe não faz nenhum processamento\n",
    "        mudancas.append(row[idx_cidade])\n",
    "        #print(row[idx_cidade], cidade)\n",
    "        \n",
    "abono_dataset[\"Cidade da residência\"] = pd.Series(data = mudancas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03662062",
   "metadata": {},
   "source": [
    "# Corrigir coluna Nível de Escolaridade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9216868d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#realiza correção do nível de escolaridade\n",
    "idx_escolaridade = 4#indice da coluna Nível de Escolaridade\n",
    "\n",
    "mud_escolaridade = {\n",
    "    \"segundo grau incompleto\": \"ensino medio incompleto\",\n",
    "    \"4a. serie do primeiro grau completa\": \"ensino fundamental incompleto\",\n",
    "    \"primeiro grau incomp.-ate a 4a.serie incomp.\": \"ensino fundamental incompleto\"\n",
    "}\n",
    "mudancas =[]\n",
    "for row in abono_dataset.itertuples():\n",
    "    if row[idx_escolaridade] in mud_escolaridade:\n",
    "        mudancas.append(mud_escolaridade[row[idx_escolaridade]])\n",
    "    else:\n",
    "         mudancas.append(row[idx_escolaridade])\n",
    "abono_dataset[\"Nível de Escolaridade\"] = pd.Series(data = mudancas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075267e2",
   "metadata": {},
   "source": [
    "# Testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa23294",
   "metadata": {},
   "outputs": [],
   "source": [
    "abono_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971df650",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
